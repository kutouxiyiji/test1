{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_try_basic_01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kutouxiyiji/test1/blob/master/Pytorch_try_basic_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ykmNeZ3RjPK",
        "colab_type": "text"
      },
      "source": [
        "Try from Numpy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmzkTLqFRdTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# ================================================================== #\n",
        "#                         Table of Contents                          #\n",
        "# ================================================================== #\n",
        "\n",
        "# 1. Basic autograd example 1               (Line 25 to 39)\n",
        "# 2. Basic autograd example 2               (Line 46 to 83)\n",
        "# 3. Loading data from numpy                (Line 90 to 97)\n",
        "# 4. Input pipline                          (Line 104 to 129)\n",
        "# 5. Input pipline for custom dataset       (Line 136 to 156)\n",
        "# 6. Pretrained model                       (Line 163 to 176)\n",
        "# 7. Save and load model                    (Line 183 to 189)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pjs_nGdRhI0",
        "colab_type": "code",
        "outputId": "2c5bb312-0954-40ea-8f84-6f7536b5ba73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# ================================================================== #\n",
        "#                     1. Basic autograd example 1                    #\n",
        "# ================================================================== #\n",
        "\n",
        "# Create tensors.\n",
        "x = torch.tensor(1., requires_grad=True)\n",
        "w = torch.tensor(2., requires_grad=True)\n",
        "b = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "# Build a computational graph.\n",
        "y = w * x + b    # y = 2 * x + 3\n",
        "\n",
        "# Compute gradients.\n",
        "y.backward()\n",
        "\n",
        "# Print out the gradients.\n",
        "print(x.grad)    # x.grad = 2 \n",
        "print(w.grad)    # w.grad = 1 \n",
        "print(b.grad)    # b.grad = 1 \n",
        "\n",
        "\n",
        "# ================================================================== #\n",
        "#                    2. Basic autograd example 2                     #\n",
        "# ================================================================== #\n",
        "\n",
        "# Create tensors of shape (10, 3) and (10, 2).\n",
        "x = torch.randn(10, 3)\n",
        "y = torch.randn(10, 2)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(3, 2)\n",
        "print ('w: ', linear.weight)  # w convert shape(10,3) to (10,2)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "# Forward pass.\n",
        "pred = linear(x)\n",
        "\n",
        "# Compute loss.\n",
        "loss = criterion(pred, y)\n",
        "print('loss: ', loss.item())\n",
        "\n",
        "# Backward pass.\n",
        "loss.backward()\n",
        "\n",
        "# Print out the gradients.\n",
        "print ('dL/dw: ', linear.weight.grad) \n",
        "print ('dL/db: ', linear.bias.grad)\n",
        "\n",
        "# 1-step gradient descent.\n",
        "optimizer.step()\n",
        "\n",
        "# You can also perform gradient descent at the low level.\n",
        "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
        "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
        "\n",
        "# Print out the loss after 1-step gradient descent.\n",
        "pred = linear(x)\n",
        "loss = criterion(pred, y)\n",
        "print('loss after 1 step optimization: ', loss.item())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.)\n",
            "tensor(1.)\n",
            "tensor(1.)\n",
            "w:  Parameter containing:\n",
            "tensor([[-0.4245,  0.3256, -0.1535],\n",
            "        [-0.1245, -0.1420,  0.1049]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([ 0.3081, -0.2722], requires_grad=True)\n",
            "loss:  1.3071843385696411\n",
            "dL/dw:  tensor([[-0.6774,  0.9695, -0.4858],\n",
            "        [ 0.0058, -0.2372,  0.0654]])\n",
            "dL/db:  tensor([ 0.4178, -0.2284])\n",
            "loss after 1 step optimization:  1.2880946397781372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LA7PH6IX7sv",
        "colab_type": "code",
        "outputId": "3381dba9-e703-414d-9446-97b29f4cb3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss.item()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2880946397781372"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XakTXEumYhmG",
        "colab_type": "code",
        "outputId": "f2a28280-18c5-48d3-8129-5d5941e03c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "for i in range(30):\n",
        "  optimizer.step()\n",
        "  pred = linear(x)\n",
        "  loss = criterion(pred,y)\n",
        "  loss.backward()\n",
        "  print('loss after {} step'.format(i), loss.item())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss after 0 step 0.8714390397071838\n",
            "loss after 1 step 0.8656208515167236\n",
            "loss after 2 step 0.8670972585678101\n",
            "loss after 3 step 0.8750539422035217\n",
            "loss after 4 step 0.888118326663971\n",
            "loss after 5 step 0.9044754505157471\n",
            "loss after 6 step 0.9220167398452759\n",
            "loss after 7 step 0.9385094046592712\n",
            "loss after 8 step 0.9517755508422852\n",
            "loss after 9 step 0.959868311882019\n",
            "loss after 10 step 0.9612309336662292\n",
            "loss after 11 step 0.9548288583755493\n",
            "loss after 12 step 0.9402453899383545\n",
            "loss after 13 step 0.9177317023277283\n",
            "loss after 14 step 0.888209879398346\n",
            "loss after 15 step 0.853226363658905\n",
            "loss after 16 step 0.8148592114448547\n",
            "loss after 17 step 0.775585412979126\n",
            "loss after 18 step 0.738115668296814\n",
            "loss after 19 step 0.7052093744277954\n",
            "loss after 20 step 0.6794812083244324\n",
            "loss after 21 step 0.6632124185562134\n",
            "loss after 22 step 0.6581805348396301\n",
            "loss after 23 step 0.6655189990997314\n",
            "loss after 24 step 0.6856166124343872\n",
            "loss after 25 step 0.7180643677711487\n",
            "loss after 26 step 0.7616546154022217\n",
            "loss after 27 step 0.8144317269325256\n",
            "loss after 28 step 0.873794436454773\n",
            "loss after 29 step 0.9366409182548523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUXWFjlTYgSE",
        "colab_type": "code",
        "outputId": "467ffc13-d07e-41b8-d9e1-1b8c40a55925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# ================================================================== #\n",
        "#                     3. Loading data from numpy                     #\n",
        "# ================================================================== #\n",
        "\n",
        "# Create a numpy array.\n",
        "x = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "# Convert the numpy array to a torch tensor.\n",
        "y = torch.from_numpy(x)\n",
        "\n",
        "# Convert the torch tensor to a numpy array.\n",
        "z = y.numpy()\n",
        "\n",
        "\n",
        "# ================================================================== #\n",
        "#                         4. Input pipline                           #\n",
        "# ================================================================== #\n",
        "\n",
        "# Download and construct CIFAR-10 dataset.\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                             train=True, \n",
        "                                             transform=transforms.ToTensor(),\n",
        "                                             download=True)\n",
        "\n",
        "# Fetch one data pair (read data from disk).\n",
        "image, label = train_dataset[0]\n",
        "print (image.size())\n",
        "print (label)\n",
        "\n",
        "# Data loader (this provides queues and threads in a very simple way).\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=64, \n",
        "                                           shuffle=True)\n",
        "\n",
        "# When iteration starts, queue and thread start to load data from files.\n",
        "data_iter = iter(train_loader)\n",
        "\n",
        "# Mini-batch images and labels.\n",
        "images, labels = data_iter.next()\n",
        "\n",
        "# Actual usage of the data loader is as below.\n",
        "for images, labels in train_loader:\n",
        "    # Training code should be written here.\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170156032/170498071 [00:23<00:00, 5377321.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPGn96EuRhZM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OJ3x3Xe4trG",
        "colab_type": "code",
        "outputId": "834b6052-ce41-4308-b6fb-cf9b6ffb95f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "# ================================================================== #\n",
        "#                5. Input pipline for custom dataset                 #\n",
        "# ================================================================== #\n",
        "\n",
        "# You should build your custom dataset as below.\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        # TODO\n",
        "        # 1. Initialize file paths or a list of file names. \n",
        "        pass\n",
        "    def __getitem__(self, index):\n",
        "        # TODO\n",
        "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
        "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
        "        # 3. Return a data pair (e.g. image and label).\n",
        "        pass\n",
        "    def __len__(self):\n",
        "        # You should change 0 to the total size of your dataset.\n",
        "        return 0 \n",
        "\n",
        "# You can then use the prebuilt data loader. \n",
        "custom_dataset = CustomDataset()\n",
        "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
        "                                           batch_size=64, \n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-706c84ca09b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n\u001b[1;32m     19\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                            shuffle=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m---> 66\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5n7U-oe41cd",
        "colab_type": "code",
        "outputId": "2cc1936b-f149-419f-a3c1-28ae4d71db8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "\n",
        "# ================================================================== #\n",
        "#                        6. Pretrained model                         #\n",
        "# ================================================================== #\n",
        "\n",
        "# Download and load the pretrained ResNet-18.\n",
        "resnet = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# If you want to finetune only the top layer of the model, set as below.\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the top layer for finetuning.\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
        "\n",
        "# Forward pass.\n",
        "images = torch.randn(64, 3, 224, 224)\n",
        "outputs = resnet(images)\n",
        "print (outputs.size())     # (64, 100)\n",
        "\n",
        "\n",
        "# ================================================================== #\n",
        "#                      7. Save and load the model                    #\n",
        "# ================================================================== #\n",
        "\n",
        "# Save and load the entire model.\n",
        "torch.save(resnet, 'model.ckpt')\n",
        "model = torch.load('model.ckpt')\n",
        "\n",
        "# Save and load only the model parameters (recommended).\n",
        "torch.save(resnet.state_dict(), 'params.ckpt')\n",
        "resnet.load_state_dict(torch.load('params.ckpt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "\n",
            "  0%|          | 0/46827520 [00:00<?, ?it/s]\u001b[A\n",
            "  4%|▍         | 1843200/46827520 [00:00<00:02, 18427931.56it/s]\u001b[A\n",
            " 25%|██▍       | 11608064/46827520 [00:00<00:01, 24355059.27it/s]\u001b[A\n",
            " 47%|████▋     | 21905408/46827520 [00:00<00:00, 31590470.05it/s]\u001b[A\n",
            " 65%|██████▍   | 30236672/46827520 [00:00<00:00, 38819597.52it/s]\u001b[A\n",
            " 82%|████████▏ | 38232064/46827520 [00:00<00:00, 45904006.02it/s]\u001b[A\n",
            "100%|██████████| 46827520/46827520 [00:00<00:00, 79355381.52it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8WWNhrJ5FuP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}